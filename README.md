"# AIAYN-transformer" 

🔬 Description
Minimal, faithful implementation of the Transformer architecture introduced in "Attention Is All You Need" (Vaswani et al., 2017). This repository excludes modern modifications (e.g., pre-layer norm, relative positional encoding, etc.) to preserve the original design for research, benchmarking, and educational purposes. Implementation by Omar Ibrahim et al.

📁 Features
Encoder-decoder Transformer architecture

Scaled dot-product attention

Multi-head attention

Position-wise feedforward networks

Learned positional encoding

Masking for autoregressive decoding

No dependencies on modern Transformer wrappers or libraries

📄 Reference
Vaswani et al., "Attention is All You Need", NeurIPS 2017
https://arxiv.org/abs/1706.03762

🛠 Requirements
Python ≥ 3.8

PyTorch ≥ 1.10 

NumPy
